global:
  enabled: true
  namespace: "vault-system"
  imagePullSecrets: []
  tlsDisable: true
  externalVaultAddr: ""

  openshift: false
  psp:
    enable: false
    annotations: |
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default,runtime/default
      apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
      seccomp.security.alpha.kubernetes.io/defaultProfileName:  runtime/default
      apparmor.security.beta.kubernetes.io/defaultProfileName:  runtime/default

  serverTelemetry:
    prometheusOperator: true

injector:
  enabled: false

server:
  enabled: true

  enterpriseLicense:
    secretName: ""
    secretKey: "license"

  image:
    repository: "hashicorp/vault"
    tag: "1.16.1"
    pullPolicy: IfNotPresent

  updateStrategyType: "OnDelete"
  logLevel: "info"
  logFormat: "json"

  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 1024Mi
      cpu: 500m

  ingress:
    enabled: true
    labels:
    annotations:
      cert-manager.io/issuer: vault-issuer
      ingress.cilium.io/loadbalancer-mode: shared
      ingress.cilium.io/service-type: ClusterIP

    ingressClassName: "cilium"
    pathType: Prefix
    activeService: true
    hosts:
      - host: vault.choigonyok.com
        paths: ["/"]
    extraPaths: []
    tls:
    - secretName: vault-crt
      hosts:
        - vault.choigonyok.com

  hostAliases: []
  route:
    enabled: false
    activeService: true
    labels: {}
    annotations: {}
    host: chart-example.local
    tls:
      termination: passthrough

  # https://developer.hashicorp.com/vault/docs/auth/kubernetes
  authDelegator:
    enabled: true

  # extraInitContainers is a list of init containers. Specified as a YAML list.
  # This is useful if you need to run a script to provision TLS certificates or
  # write out configuration files in a dynamic way.
  extraInitContainers: null
    # # This example installs a plugin pulled from github into the /usr/local/libexec/vault/oauthapp folder,
    # # which is defined in the volumes value.
    # - name: oauthapp
    #   image: "alpine"
    #   command: [sh, -c]
    #   args:
    #     - cd /tmp &&
    #       wget https://github.com/puppetlabs/vault-plugin-secrets-oauthapp/releases/download/v1.2.0/vault-plugin-secrets-oauthapp-v1.2.0-linux-amd64.tar.xz -O oauthapp.xz &&
    #       tar -xf oauthapp.xz &&
    #       mv vault-plugin-secrets-oauthapp-v1.2.0-linux-amd64 /usr/local/libexec/vault/oauthapp &&
    #       chmod +x /usr/local/libexec/vault/oauthapp
    #   volumeMounts:
    #     - name: plugins
    #       mountPath: /usr/local/libexec/vault

  extraContainers: null
  shareProcessNamespace: false
  extraArgs: ""
  extraPorts: null
  readinessProbe:
    enabled: true
    port: 8200
    failureThreshold: 2
    initialDelaySeconds: 5
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 3
  livenessProbe:
    enabled: false
    execCommand: []
    path: "/v1/sys/health?standbyok=true"
    port: 8200
    failureThreshold: 2
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 3

  terminationGracePeriodSeconds: 10
  preStopSleepSeconds: 5

  # Used to define commands to run after the pod is ready.
  # This can be used to automate processes such as initialization
  # or boostrapping auth methods.
  postStart:
  - /bin/sh
  - -c
  - TEST=$(echo $(vault operator init -n 1 -t 1) | awk '/Unseal Key 1 / {print $4}')
  - vault operator unseal $TEST

  # - vault operator unseal $TEST

  # - vault operator init -n 1 -t 1 > /vault/tokens/root
  # - vault operator unseal $(cat /vault/tokens/root  | awk '/Unseal Key 1 / {print $4}')
  # - cat /vault/tokens/root | awk '/Initial Root Token / {print $4}' > /vault/tokens/root

  extraEnvironmentVars: {}
  extraSecretEnvironmentVars: []
  extraVolumes: []
    # - type: secret (or "configMap")
    #   name: my-secret
    #   path: null # default is `/vault/userconfig`

  volumes: null
  #   - name: plugins
  #     emptyDir: {}

  volumeMounts: null
  #   - mountPath: /usr/local/libexec/vault
  #     name: plugins
  #     readOnly: true

  affinity: 
  # affinity: |
  #   podAntiAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       - labelSelector:
  #           matchLabels:
  #             app.kubernetes.io/name: {{ template "vault.name" . }}
  #             app.kubernetes.io/instance: "{{ .Release.Name }}"
  #             component: server
  #         topologyKey: kubernetes.io/hostname

  topologySpreadConstraints: []
  tolerations: []
  nodeSelector:
    node-type: worker

  networkPolicy:
    enabled: true
    egress: []
    ingress:
      - from:
        - namespaceSelector: {}
        ports:
        - port: 8200
          protocol: TCP
        - port: 8201
          protocol: TCP

  priorityClassName: ""
  extraLabels: {}
  annotations: {}
  configAnnotation: false

  service:
    enabled: true
    active:
      enabled: true
      annotations: {}
    standby:
      enabled: true
      annotations: {}
    # If enabled, the service selectors will include `app.kubernetes.io/instance: {{ .Release.Name }}`
    # When disabled, services may select Vault pods not deployed from the chart.
    # Does not affect the headless vault-internal service with `ClusterIP: None`
    instanceSelector:
      enabled: true
    # clusterIP: None # for headless service
    # type: ClusterIP

    ipFamilyPolicy: ""
    ipFamilies: []
    publishNotReadyAddresses: true
    externalTrafficPolicy: Cluster

    activeNodePort: 30001
    standbyNodePort: 30002

    port: 8200
    targetPort: 8200
    annotations: {}

  dataStorage:
    enabled: true
    # Size of the PVC created
    size: 10Gi
    mountPath: "/vault/data"
    storageClass: ceph-fs
    accessMode: ReadWriteOnce
    annotations: {}
    labels: {}

  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain

  auditStorage:
    enabled: true
    size: 10Gi
    mountPath: "/vault/audit"
    storageClass: ceph-fs
    accessMode: ReadWriteOnce
    annotations: {}
    labels: {}

  dev:
    enabled: false
    devRootToken: "root"

  standalone:
    enabled: false

    config: |
      ui = true

      listener "tcp" {
        tls_disable = 1
        tls_disable_client_certs = true
        address = "[::]:8200"
        cluster_address = "[::]:8201"
        # Enable unauthenticated metrics access (necessary for Prometheus Operator)
        #telemetry {
        #  unauthenticated_metrics_access = "true"
        #}
      }
      storage "file" {
        path = "/vault/data"
      }

      # Example configuration for using auto-unseal, using Google Cloud KMS. The
      # GKMS keys must already exist, and the cluster must have a service account
      # that is authorized to access GCP KMS.
      #seal "gcpckms" {
      #   project     = "vault-helm-dev"
      #   region      = "global"
      #   key_ring    = "vault-helm-unseal-kr"
      #   crypto_key  = "vault-helm-unseal-key"
      #}

      # Example configuration for enabling Prometheus metrics in your config.
      #telemetry {
      #  prometheus_retention_time = "30s"
      #  disable_hostname = true
      #}

  ha:
    enabled: true
    replicas: 3

    # Set the api_addr configuration for Vault HA
    # See https://developer.hashicorp.com/vault/docs/configuration#api_addr
    # If set to null, this will be set to the Pod IP Address
    apiAddr: https://vault.choigonyok.com

    # Set the cluster_addr confuguration for Vault HA
    # See https://developer.hashicorp.com/vault/docs/configuration#cluster_addr
    # If set to null, this will be set to https://$(HOSTNAME).{{ template "vault.fullname" . }}-internal:8201
    clusterAddr: http://vault-server-internal.vault-system.svc.cluster.local:8201

    raft:
      enabled: true
      setNodeId: true

      # Note: Configuration files are stored in ConfigMaps so sensitive data
      # such as passwords should be either mounted through extraSecretEnvironmentVars
      # or through a Kube secret.  For more information see:
      # https://developer.hashicorp.com/vault/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurations
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          tls_disable_client_certs = true
          address = "[::]:8200"
          cluster_address = "[::]:8201"
          # Enable unauthenticated metrics access (necessary for Prometheus Operator)
          telemetry {
            unauthenticated_metrics_access = "true"
          }
        }

        storage "raft" {
          path = "/vault/data"
        }

        service_registration "kubernetes" {}

    config: ""

    disruptionBudget:
      enabled: true
      maxUnavailable: 2

  serviceAccount:
    create: true
    name: "vault-sa"
    createSecret: true
    annotations: {}
    extraLabels: {}
    serviceDiscovery:
      enabled: true

  statefulSet:
    annotations: {}

    # Set the pod and container security contexts.
    # If not set, these will default to, and for *not* OpenShift:
    # pod:
    #   runAsNonRoot: true
    #   runAsGroup: {{ .Values.server.gid | default 1000 }}
    #   runAsUser: {{ .Values.server.uid | default 100 }}
    #   fsGroup: {{ .Values.server.gid | default 1000 }}
    # container:
    #   allowPrivilegeEscalation: false
    #
    # If not set, these will default to, and for OpenShift:
    # pod: {}
    # container: {}
    securityContext:
      pod: {}
      container: {}

  # Should the server pods run on the host network
  hostNetwork: false

ui:
  enabled: true
  publishNotReadyAddresses: true
  activeVaultPodOnly: true
  serviceType: "ClusterIP"
  serviceNodePort: null
  externalPort: 8200
  targetPort: 8200

  serviceIPFamilyPolicy: ""
  serviceIPFamilies: []
  externalTrafficPolicy: Cluster
  annotations: {}

csi:
  # True if you want to install a secrets-store-csi-driver-provider-vault daemonset.
  #
  # Requires installing the secrets-store-csi-driver separately, see:
  # https://github.com/kubernetes-sigs/secrets-store-csi-driver#install-the-secrets-store-csi-driver
  #
  # With the driver and provider installed, you can mount Vault secrets into volumes
  # similar to the Vault Agent injector, and you can also sync those secrets into
  # Kubernetes secrets.
  enabled: false

  image:
    repository: "hashicorp/vault-csi-provider"
    tag: "1.4.2"
    pullPolicy: IfNotPresent

  # volumes is a list of volumes made available to all containers. These are rendered
  # via toYaml rather than pre-processed like the extraVolumes value.
  # The purpose is to make it easy to share volumes between containers.
  volumes: null
  # - name: tls
  #   secret:
  #     secretName: vault-tls

  # volumeMounts is a list of volumeMounts for the main server container. These are rendered
  # via toYaml rather than pre-processed like the extraVolumes value.
  # The purpose is to make it easy to share volumes between containers.
  volumeMounts: null
  # - name: tls
  #   mountPath: "/vault/tls"
  #   readOnly: true

  resources: {}
  # resources:
  #   requests:
  #     cpu: 50m
  #     memory: 128Mi
  #   limits:
  #     cpu: 50m
  #     memory: 128Mi

  # Override the default secret name for the CSI Provider's HMAC key used for
  # generating secret versions.
  hmacSecretName: ""

  # Settings for the daemonSet used to run the provider.
  daemonSet:
    updateStrategy:
      type: RollingUpdate
      maxUnavailable: ""
    # Extra annotations for the daemonSet. This can either be YAML or a
    # YAML-formatted multi-line templated string map of the annotations to apply
    # to the daemonSet.
    annotations: {}
    # Provider host path (must match the CSI provider's path)
    providersDir: "/etc/kubernetes/secrets-store-csi-providers"
    # Kubelet host path
    kubeletRootDir: "/var/lib/kubelet"
    # Extra labels to attach to the vault-csi-provider daemonSet
    # This should be a YAML map of the labels to apply to the csi provider daemonSet
    extraLabels: {}
    # security context for the pod template and container in the csi provider daemonSet
    securityContext:
      pod: {}
      container: {}

  pod:
    # Extra annotations for the provider pods. This can either be YAML or a
    # YAML-formatted multi-line templated string map of the annotations to apply
    # to the pod.
    annotations: {}

    # Toleration Settings for provider pods
    # This should be either a multi-line string or YAML matching the Toleration array
    # in a PodSpec.
    tolerations: []

    # nodeSelector labels for csi pod assignment, formatted as a multi-line string or YAML map.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    # nodeSelector:
    #   beta.kubernetes.io/arch: amd64
    nodeSelector: {}

    # Affinity Settings
    # This should be either a multi-line string or YAML matching the PodSpec's affinity field.
    affinity: {}

    # Extra labels to attach to the vault-csi-provider pod
    # This should be a YAML map of the labels to apply to the csi provider pod
    extraLabels: {}

  agent:
    enabled: false
    extraArgs: []

    image:
      repository: "hashicorp/vault"
      tag: "1.16.1"
      pullPolicy: IfNotPresent

    logFormat: standard
    logLevel: info

    resources:
      requests:
        memory: 128i
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 250m

  priorityClassName: ""

  serviceAccount:
    annotations: {}
    extraLabels: {}

  readinessProbe:
    failureThreshold: 2
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5
  livenessProbe:
    failureThreshold: 2
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

  debug: false

  # Pass arbitrary additional arguments to vault-csi-provider.
  # See https://developer.hashicorp.com/vault/docs/platform/k8s/csi/configurations#command-line-arguments
  # for the available command line flags.
  extraArgs: []

# Vault is able to collect and publish various runtime metrics.
# Enabling this feature requires setting adding `telemetry{}` stanza to
# the Vault configuration. There are a few examples included in the `config` sections above.
#
# For more information see:
# https://developer.hashicorp.com/vault/docs/configuration/telemetry
# https://developer.hashicorp.com/vault/docs/internals/telemetry
serverTelemetry:
  serviceMonitor:
    enabled: true
    selectors: {}
    interval: 30s
    scrapeTimeout: 10s

  prometheusRules:
      # The Prometheus operator *must* be installed before enabling this feature,
      # if not the chart will fail to install due to missing CustomResourceDefinitions
      # provided by the operator.

      # Deploy the PrometheusRule custom resource for AlertManager based alerts.
      # Requires that AlertManager is properly deployed.
      enabled: false

      # Selector labels to add to the PrometheusRules.
      # When empty, defaults to:
      #  release: prometheus
      selectors: {}

      # Some example rules.
      rules: []
      #  - alert: vault-HighResponseTime
      #    annotations:
      #      message: The response time of Vault is over 500ms on average over the last 5 minutes.
      #    expr: vault_core_handle_request{quantile="0.5", namespace="mynamespace"} > 500
      #    for: 5m
      #    labels:
      #      severity: warning
      #  - alert: vault-HighResponseTime
      #    annotations:
      #      message: The response time of Vault is over 1s on average over the last 5 minutes.
      #    expr: vault_core_handle_request{quantile="0.5", namespace="mynamespace"} > 1000
      #    for: 5m
      #    labels:
      #      severity: critical
